{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BafKS1rR6Mkc"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h1><span style=\"color:green\"> Under-Graduate Research Internship Program (UGRIP) - 2024 <br> Lab 01 - Part B </span><h1>\n",
    "\n",
    "<h2><span style=\"color:green\"> Classification Algorithms: K-Nearest Neighbors (KNN), SVMs, Decision Trees and Random Forests</span><h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session we're going to take a closer look at specialized classification algorithms to understand their behavior and investigate their properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhLr4lNpYPH3"
   },
   "source": [
    "# 1. k-Nearest Neighbors (kNN)\n",
    "\n",
    "k-Nearest Neighbors is a simple supervised machine learning algorithm which can be used for both classification and regression tasks. Unlike most other estimators, it doesn't attempt to fit or learn any model parameters i.e. it is model-free. Rather, it stores the training data internally (formally they are called \"exemplars\"). That's right; no training involved.\n",
    "\n",
    "When you want to use it on new data (formally called \"inference time\") and you present it with new data, it simply compares the new data point to all the exemplars it has stored. Once the \"nearest neighbor\" of this unknown point has been located, then the new point takes on the label of this nearest neighbor.\n",
    "\n",
    "Now, this classifier has only one \"hyperparameter\" (a hyperparameter is a user-specified number that controls the operation of the estimator/model, but is not learnt from data), which is $k$; the number of neighbors involved in the labelling process. In our explanation above, we consider only the (one) nearest neighbor, so that is the operation of a 1-Nearest Neighbor classifier. We can also use more neighbors e.g. $k=3$, in which case the label of the new, unseen point will be the majority vote of the labels of the 3 nearest neighbors and so on.\n",
    "\n",
    "As usual, sklearn comes with an implementation of the k-Nearest Neighbor classifier in the `sklearn.neighbors` module. Let us import it and other relevant imports below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrEE1JlWvqe6"
   },
   "outputs": [],
   "source": [
    "import numpy as np                 # linear algebra and numerical computing\n",
    "import matplotlib.pyplot as plt    # for data visualization \n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us consider the famous Iris dataset. This dataset consists of 4 measurements taken for several Iris plants which belong to 3 species. We will use this dataset to explore the properties/behavior of the kNN. \n",
    "\n",
    "Go ahead and load the data from the `sklearn.datasets` module using the `load_iris()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Implement Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, go ahead to separate the data into features and targets. <ins> Select the first two features in the dataset</ins>, and make a training and test split (75%/25% ratio). Set the random seed to 1000 for reproducibility (by setting the `random_state` argument to 1000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Here\n",
    "features, labels = ..., ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_FKKA7mcBV8"
   },
   "source": [
    "Finally, go ahead fit a 1-Nearest Neighbor classifier to it and evaluate the performance of the classifier using the accuracy evaluation metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1663239704039,
     "user": {
      "displayName": "Asif Hanif",
      "userId": "07364248590185418247"
     },
     "user_tz": -240
    },
    "id": "QSn9NpBsyrSH",
    "outputId": "46d7e737-1d23-441c-ae42-d80ccf8d7f16"
   },
   "outputs": [],
   "source": [
    "# Implement Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think of the k-NN performance? Do you think this is a good classifier?\n",
    "\n",
    "## Effect of $k$\n",
    "\n",
    "Now, you will do an experiment to explore the effect of $k$ on the classification accuracy of the classifier. You need to do the following:\n",
    "- Consider values from $k$ = 1 to 11\n",
    "    - For each $k$ value, initialize and fit a kNN with the number of neighbors set to the current $k$ value\n",
    "    - Compute the accuracy at this value of $k$\n",
    "\n",
    "When finished, plot the classifier accuracy as a function of $k$. This is just a fancy way of saying put the value of $k$ on the x-axis, and the corresponding classifier accuracy on the y-axis. Bonus points for nice visual presentation of results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1663239704040,
     "user": {
      "displayName": "Asif Hanif",
      "userId": "07364248590185418247"
     },
     "user_tz": -240
    },
    "id": "IbCWGKiXeS2Y",
    "outputId": "287b584d-b729-4891-b103-be5823ac1f79"
   },
   "outputs": [],
   "source": [
    "# Implement Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will revisit this phenomenon shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Scaling on the k-NN\n",
    "\n",
    "Let us try a simple experiment. What happens if we multiply one feature to make it much larger than the other? Would the k-NN still work well?\n",
    "To investigate this, we will multiply one feature by 1000, then split the data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1663239704041,
     "user": {
      "displayName": "Asif Hanif",
      "userId": "07364248590185418247"
     },
     "user_tz": -240
    },
    "id": "93PxFGbpf8Ic",
    "outputId": "a709aee2-5ec1-4e31-a53c-6e6dbee4be0e"
   },
   "outputs": [],
   "source": [
    "features[:, 0] *= 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let repeat the data splitting process (using our fixed random seed for reproducibility):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1663239704041,
     "user": {
      "displayName": "Asif Hanif",
      "userId": "07364248590185418247"
     },
     "user_tz": -240
    },
    "id": "fFuvu3Uw5MYy",
    "outputId": "1b37701a-c18c-4f29-f6f0-8b4cce59eefd"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, repeat the plot we made above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1663239704042,
     "user": {
      "displayName": "Asif Hanif",
      "userId": "07364248590185418247"
     },
     "user_tz": -240
    },
    "id": "HjQrYER-4zlM",
    "outputId": "feea29cb-8991-48f8-aadc-dec3f3deedf0"
   },
   "outputs": [],
   "source": [
    "# Implement Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this seems to have harmed the kNN severely. This is because of how it works, as it relies on something called a distance function.\n",
    "\n",
    "## Distance Functions\n",
    "\n",
    "Recall that a kNN assigns a new point to the label of its nearest exemplar(s). But how can we determine the concept of nearness?\n",
    "From an ML perspective, we need to have a way to measure the nearness or closeness of two points i.e. to put a number on it. We can rephrase that and say that we need a way to measure _distance_ between the two points. \n",
    "\n",
    "This latter formulation is nicer because it has better properties both intuitively and mathematically. For instance, if two points have a distance of 0 between them, then they are the same point. If we had used the other definition, then it would be hard to put a hard number of the degree of closeness of two points.\n",
    "\n",
    "Therefore, this motivates a distance function. A distance function takes two points (or more concretely, the numbers/features specifying two points), and returns a quantitative measure of the distance between them. For instance, you have dealt with Euclidean distance in your math courses, which says that the distance between two points $P_1 = (x_1, y_1)$ and $P_2 = (x_2, y_2)$ in Euclidean space is given by:\n",
    "$$\n",
    "    D(P_1, P_2) = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incidentally, the kNN relies on the exact same Euclidean distance function for its operation. \n",
    "\n",
    "Now that you know this, can you guess why increasing the magnitude of one of the features negatively affected its operation? More importantly, can you suggest potential ways to fix the problem?\n",
    "\n",
    "There are many other distance functions, each of which have distinct properties. For example, cosine distance is one type of distance function which measures the _angle_ between two points (actually two vectors with a common point of reference). For this reason, cosine distance is what is termed as being _scale-invariant_. \n",
    "\n",
    "Your next task is to prove this by:\n",
    "- reloading the iris data and selecting its first two columns/features again\n",
    "- fitting a 1-NN (using cosine distance) to it and computing its accuracy\n",
    "- multiplying one of the features of the data, and refitting the 1-NN to this rescaled data\n",
    "- and recomputing the accuracy\n",
    "- finally, compare the two accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe?\n",
    "\n",
    "# Decision Boundaries\n",
    "\n",
    "You can think of a classifier as basically trying to find a way to separate the items from different classes. For instance, consider a simple case where you have data belonging to some classes in a 2D space (i.e. having two features, so you can plot them on a sheet of paper). We can even consider our dataset from above accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "features, labels = iris.data[:, :2], iris.target\n",
    "plt.scatter(features[:, 0], features[:,1], c=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you draw a line that perfectly separates them?\n",
    "\n",
    "All classifiers basically try to achieve that goal using principled means. For instance, logistic regression (which is a linear classifier) tries to find lines to separate the data from each other by adjusting its weights. The kNN on the other hand doesn't explicitly try to find a line, but instead stumbles upon one or induces one based on the data distribution. \n",
    "\n",
    "It would be nice to be able to see how each classifier goes about drawing these separating lines (which are termed \"Decision Boundaries\"), as that will tell us what the classifier is thinking.\n",
    "\n",
    "In 2D (such as in our case here), it is fairly easy to do this. Sklearn comes with functionality to draw the decision boundary of a classifier by using the `DecisionBoundaryDisplay` class (from the `sklearn.inspection` module). It is fairly complex to use, but we can easily create one using its `from_estimator()` function, which we will then use.\n",
    "\n",
    "To kill two birds with one stone, let us now create two kNNs: one with $k = 1$ and the other with $k = 9$ (from our graph earlier), and see the differences in their behavior via their decision boundary plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, labels, train_size=0.75, random_state=1000)\n",
    "knn_1 = KNeighborsClassifier(1).fit(X_train, Y_train)\n",
    "knn_11 = KNeighborsClassifier(9).fit(X_train, Y_train)\n",
    "\n",
    "predictions_1 = knn_1.predict(X_test)\n",
    "predictions_11 = knn_11.predict(X_test)\n",
    "\n",
    "fig, axes = plt.subplots(1,2)\n",
    "\n",
    "disp1 = DecisionBoundaryDisplay.from_estimator(\n",
    "    knn_1, X_train, response_method=\"predict\",\n",
    "    alpha=0.5,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].scatter(X_train[:, 0], X_train[:, 1], c=Y_train, edgecolor=\"k\")\n",
    "\n",
    "disp1 = DecisionBoundaryDisplay.from_estimator(\n",
    "    knn_11, X_train, response_method=\"predict\",\n",
    "    alpha=0.5,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].scatter(X_train[:, 0], X_train[:, 1], c=Y_train, edgecolor=\"k\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weaknesses of the kNN\n",
    "\n",
    "Now that we have seen and experimented with the kNN, can you come up with its weaknesses? You get one for free i.e. sensitive to feature scales. But are there others you can think of?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNHUOHIQjTY8"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXh2CoYXvHS_"
   },
   "source": [
    "# 2. Support Vector Machines\n",
    "\n",
    "Support vector machines (or SVMs) are another very popular algorithm for classification (although they can also be used for regression as well). Unlike the other classifiers we've seen, they take a completely different approach while taking some of the limitations we've seen so far into consideration e.g. linear assumptions.\n",
    "\n",
    "Rather than worry about all the training data, SVMs instead focus on making decision boundaries that are maximally separated from the borders of the two classes. In other words, they focus only on the training data points that are closest to the decision boundary. It is these \"extreme\" points that are called Support Vectors. \n",
    "\n",
    "## Kernel Functions\n",
    "\n",
    "SVMs also realize that it is sometimes hard to linearly separate the data classes because sometimes the boundaries can be irregular. As a result, SVMs allow for the use of data projection functions internally. The assumption is: if the data cannot be linearly separated in its native/natural space, then maybe it can be more easily separated in a higher dimensional space. Intuitively, think about it like this: it might be hard to distinguish between two people (say twins to make it hard) based on their heights and weights alone (since they are physically similar). But if we add new features e.g. the favorite animes of each twin, the names of their best friends, etc, then they will be easier to distinguish. \n",
    "\n",
    "SVMs achieve this by: taking the data points and passing them through a transformation function to increase their dimensionality, then trying to separate them in this new high dimensional form by estimating similarity in a kernel-specific way. For simplicity we can call these special projection + similarity-measuring functions \"kernel functions\". The exact kernel function used is set by the user i.e. it is a hyperparameter, not a learned parameter. We will explore the different kernel functions SVMs offer to see their differences in the following sections.\n",
    "\n",
    "## A Simple Experiment with Kernels\n",
    "\n",
    "But first, let us try a small experiment with the SVM and our Iris classification problem from above. We will create four SVM classifiers, each one with a different kernel, fit them to the same data (`X_train` and `Y_train` from above) and compute their accuracy on the same test data (`X_test` and `Y_test`). This will give us the comparative performance at a glance of each of these kernels relative to each other.\n",
    "\n",
    "To keep the code clean, we will use the `score()` function of each estimator. Every estimator - whether classification or regression - implements a `score()` function, which computes a default evaluation metric on the supplied data. For regression models, you get the R^2 score which measures correlation*. For classifiers, you get accuracy. Using this function we can cut down on code a little bit and get accuracy quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "a = SVC(kernel='linear').fit(X_train, Y_train).score(X_test, Y_test)\n",
    "b = SVC(kernel='rbf').fit(X_train, Y_train).score(X_test, Y_test)\n",
    "c = SVC(kernel='sigmoid').fit(X_train, Y_train).score(X_test, Y_test)\n",
    "d = SVC(kernel='poly').fit(X_train, Y_train).score(X_test, Y_test)\n",
    "\n",
    "print(a,b,c,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the SVM easily rivals the k-NN with very little effort. \n",
    "\n",
    "Now, let us dig a bit deeper into these kernel functions. Consider the toy dataset below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(\n",
    "    [\n",
    "        [0.4, -0.7],\n",
    "        [-1.5, -1.0],\n",
    "        [-1.4, -0.9],\n",
    "        [-1.3, -1.2],\n",
    "        [-1.1, -0.2],\n",
    "        [-1.2, -0.4],\n",
    "        [-0.5, 1.2],\n",
    "        [-1.5, 2.1],\n",
    "        [1.0, 1.0],\n",
    "        [1.3, 0.8],\n",
    "        [1.2, 0.5],\n",
    "        [0.2, -2.0],\n",
    "        [0.5, -2.4],\n",
    "        [0.2, -2.3],\n",
    "        [0.0, -2.7],\n",
    "        [1.3, 2.1],\n",
    "    ]\n",
    ")\n",
    "\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "plt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]\n",
    "plt.scatter(X[:, 0], X[:, 1], s=150, c=y, edgecolors=\"k\")\n",
    "plt.title(\"Samples in two-dimensional feature space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to before, we will look at the decision boundary of the SVM. But what we will do is to swap out different kernels and see how the choice of kernel affects the shape of the decision boundary. For simplicity, we will define a function that accepts our target kernel as input and produces the corresponding decision boundary as output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "\n",
    "def plot_training_data_with_decision_boundary(\n",
    "    kernel, ax=None, long_title=True, support_vectors=True\n",
    "):\n",
    "    # Train the SVC\n",
    "    clf = svm.SVC(kernel=kernel, gamma=2).fit(X, y)\n",
    "\n",
    "    # Settings for plotting\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(4, 3))\n",
    "    x_min, x_max, y_min, y_max = -3, 3, -3, 3\n",
    "    ax.set(xlim=(x_min, x_max), ylim=(y_min, y_max))\n",
    "\n",
    "    # Plot decision boundary and margins\n",
    "    common_params = {\"estimator\": clf, \"X\": X, \"ax\": ax}\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        **common_params,\n",
    "        response_method=\"predict\",\n",
    "        plot_method=\"pcolormesh\",\n",
    "        alpha=0.3,\n",
    "    )\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        **common_params,\n",
    "        response_method=\"decision_function\",\n",
    "        plot_method=\"contour\",\n",
    "        levels=[-1, 0, 1],\n",
    "        colors=[\"k\", \"k\", \"k\"],\n",
    "        linestyles=[\"--\", \"-\", \"--\"],\n",
    "    )\n",
    "\n",
    "    if support_vectors:\n",
    "        # Plot bigger circles around samples that serve as support vectors\n",
    "        ax.scatter(\n",
    "            clf.support_vectors_[:, 0],\n",
    "            clf.support_vectors_[:, 1],\n",
    "            s=150,\n",
    "            facecolors=\"none\",\n",
    "            edgecolors=\"k\",\n",
    "        )\n",
    "\n",
    "    # Plot samples by color and add legend\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, s=30, edgecolors=\"k\")\n",
    "    ax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Classes\")\n",
    "    if long_title:\n",
    "        ax.set_title(f\" Decision boundaries of {kernel} kernel in SVC\")\n",
    "    else:\n",
    "        ax.set_title(kernel)\n",
    "\n",
    "    if ax is None:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that done, we can now explore each kernel separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_data_with_decision_boundary(\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear kernel basically uses the raw feature space as-is i.e. doesn't actually involve any projection. As you can see, it draws a straight line that isn't any different than what anyone else might have drawn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poly(nomial) Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_data_with_decision_boundary(\"poly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the polynomial kernel is able to come up with a flexible decision boundary. This is because it measures similarity differently than the linear kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_data_with_decision_boundary(\"rbf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RBF kernel comes with decision boundaries that are shaped to the distribution of the data. This makes it very nice for irregular data distributions (which are very common in practice). Furthermore, it works equally well for well-behaved data. Because of its performance it is actually the default kernel for SVMs in sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_data_with_decision_boundary(\"sigmoid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid kernel is closely related to the linear kernel. What it tries to do is to form an S-shaped decision boundary, which naturally only works if the data can fit in such a specific envelope. Because of this weakness, the sigmoid kernel is very rarely used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The XOR Problem\n",
    "\n",
    "There is a common problem in the ML community called the XOR problem, which has to do with a problem that is not linearly separable (recall that linear separability simply means you can divide the classes from each other using a straight line). Let us see how well the SVM performs on this problem with different kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.linspace(-3, 3, 500), np.linspace(-3, 3, 500))\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(300, 2)\n",
    "y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)\n",
    "\n",
    "_, ax = plt.subplots(2, 2, figsize=(8, 8))\n",
    "args = dict(long_title=False, support_vectors=False)\n",
    "plot_training_data_with_decision_boundary(\"linear\", ax[0, 0], **args)\n",
    "plot_training_data_with_decision_boundary(\"poly\", ax[0, 1], **args)\n",
    "plot_training_data_with_decision_boundary(\"rbf\", ax[1, 0], **args)\n",
    "plot_training_data_with_decision_boundary(\"sigmoid\", ax[1, 1], **args)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are your thoughts? Which kernel do you think does best? Can you justify why?\n",
    "\n",
    "We can even go a step further and plot the decision boundary of an RBF-SVM on our original Iris classification task. This is left as a quick exercise to you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Decision Trees and Random Forests\n",
    "\n",
    "Now we will explore our last classifier of this session. Decision trees are another type of classifier which take a different approach to classification. Rather than try to find a line or \"optimal\" decision boundary, they instead tackle the classification in a sequential manner. \n",
    "\n",
    "Say for example, you have a problem with X features. The Decision Tree (DT) classifier will randomly select one feature and then find a threshold that best separates the data into two groups e.g positive and negative. If you think of this graphically, data comes into a node, then there are two branches or routes coming out of it. Then for each (new) branch, the DT classifier randomly selects another feature to separate the data in that branch into two sets, and so on and so forth (recursive partitioning).\n",
    "\n",
    "As the depth of the tree grows, the data at each node becomes more and more pure/homogenous. Eventually, the DT stops growing new nodes and branches when it is able to perfectly isolate a particular class in a branch (a terminal branch is technically called a leaf) or it reaches the maximum preset depth (a hyperparameter).\n",
    "\n",
    "As you can see, DTs can be thought of in visual terms very easily. This makes them very easy to interpret (since you can draw what the tree is doing at each step in sequence, and they mimic human reasoning in the way they work), as we will see shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example\n",
    "\n",
    "Now, let us begin by applying a decision tree to our Iris classification problem above, both to see its performance and also to look at the sort of decision boundary it creates. We will use the DT implementation in sklearn (under the `sklearn.tree` module):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier(criterion='entropy')\n",
    "dtc.fit(X_train, Y_train)\n",
    "print(dtc.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the DT works better than the kNN, but doesn't beat the SVM. However, it is interpretable, so we can examine its operation by visualizing the tree's nodes. To do that, we simply use the `plot_tree()` function in the `sklearn.tree` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plot_tree(dtc, filled=True)\n",
    "plt.savefig('tree.png', dpi=500) #uncomment to save a figure that might be easier to see\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at its decision boundary to compare with the others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = dtc.predict(X_test)\n",
    "\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    dtc, X_train, response_method=\"predict\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, edgecolor=\"k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will quickly observe that the DT makes decision boundaries that are axis-aligned i.e. parallel to either the x- or y-axis. Based on the way it works i.e. finding thresholds per feature, can you intuit why this is the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weaknesses of the DT\n",
    "\n",
    "You may have noticed that the DT, even for our fairly simple 2D problem, made a fairly complex tree. This is because it is trying very hard to perfectly sort/classify the training data perfectly. While this might seem like a good idea, it turns out that it makes the DT fairly weak. This is because the DT basically has a tendency to be perfect for the training data, but as a result, be unable to work well for new, unseen data (because the thresholds it learned were meant for the training data and not necessarily for new, unseen data).\n",
    "This phenomenon is called \"overfitting\", and will be explored in the last session for today.\n",
    "\n",
    "Furthermore, depending on the initial feature selected, you can end up with vastly different trees (with vastly different performances), which definitely isn't very good news.\n",
    "\n",
    "Wouldn't it be nice if we could somehow deal with these two problems? Enter the Random Forest Classifier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "As the name suggests, a Random Forest (RF) classifier is a forest - made up of many individual trees. Random Forests were created to tackle the problems with DTs. \n",
    "\n",
    "If you are familiar with the phrase that \"two heads are better than one\", the Random Forests classifier embodies this reasoning. Rather than relying on a single tree (which can overfit), it incorporates multiple trees, each of them grown differently/independently of the other. To make the constituent trees even more diverse, Random Forests carry out something called Bootstrap Aggregation (also called Bagging). Basically, they randomly sample the training data, then grow each decision tree within them (the number is a hyperparameter) on a different random sample of the data. Therefore each decision tree sees a different selection of the training data, and individually selects its own set of features to split on, making each of the trees very different from each other. This allows for better diversity of opinions and is very useful in decision making (see below).\n",
    "\n",
    "When it is time to take decisions on new data (i.e. at inference time), the Random Forest classifier allows each DT inside it to make its own decision, then takes a majority vote across all its DTs. This is a powerful idea because it means that even some of the individual DTs overfit, the ones that do not can somehow compensate in the decision making process, which makes the RF classifier generally much less prone to overfitting. Furthermore, by relying on multiple trees, we can eliminate the \"flakiness\" of the overall classifier since the RF is listening to many trees not just one potentially flaky one. This is how the RF solves the problems of the DT classifier.\n",
    "\n",
    "PS: The sklearn implementation of RFs is found in `sklearn.ensemble.RandomForestClassifier`. Fitting it means fitting the DTs inside it, which will definitely take some time. But since each DT is independent of the other, they can be fit in parallel. This is achieved by setting the parameter `n_jobs=-1` when you create the RF classifier object.\n",
    "\n",
    "PPS: A Random Forest classifier is called an Ensemble classifier because it is a big classifier made out of many smaller classifiers working together.\n",
    "\n",
    "You can actually also plot the constituents of the RF classifier, but the figure will be quite complex and large so we will avoid doing that. Rather, we will try an algorithm shootout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The Great Algorithm Shootout\n",
    "\n",
    "Now, we are going to evaluate the classifier algorithms we have seen so far on a complex problem, in order to see how well each of them perform. You will be responsible for implementing this task in code and recording the results.\n",
    "\n",
    "Your candidates are:\n",
    "- 1-NN\n",
    "- SVMs: RBF\n",
    "- DT\n",
    "- RF (300 trees)\n",
    "\n",
    "Your target task is classification on the UJIIndoorLoc dataset (details [here](https://archive.ics.uci.edu/dataset/310/ujiindoorloc)). This is a very large dataset which consists of WiFi signal strengths and the corresponding floor/building they were recorded in. Your job is to test each of these classifiers on this task, reporting both their training and testing accuracy.\n",
    "\n",
    "In this case, the dataset has already been pre-split into training (trainingData.csv) and testing (validationData.csv) portions. But to make things more interesting, we are going to train on the testing data, and test on the training data.\n",
    "\n",
    "The features of interest are in columns 0 to 519, and the target labels are in column 522.\n",
    "\n",
    "Hint: It might be a good idea to write a function to do this, since the pipeline is the same for all the classifiers. All that changes is the _type_ of classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you observe from the results?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "13o2FO4PQT-LxCZlAs5wA9wCK2ecFZoXC",
     "timestamp": 1663243488430
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
